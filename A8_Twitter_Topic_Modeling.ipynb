{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A8_Twitter_Topic_Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7w9ZZCSfm1i",
        "colab_type": "text"
      },
      "source": [
        "## Assignment 8: Twitter Topic Modeling with Non-negative Matrix Factorization.\n",
        "\n",
        "### Due: Thursday, June 4th, 11:59 pm on Gradescope.\n",
        "\n",
        "In this homework you will practice extracting topics from tweets using the relatively simple Non-negative Matrix Factorization method. This method assumes every tweet is a combination of several topics weighted by their prevailance in the text. This approach in fact finds a low-dimensional representation of the tweets (through the topic weights). \n",
        "\n",
        "The dataset is obtained from https://www.kaggle.com/smid80/coronavirus-covid19-tweets-late-april?select=2020-04-30+Coronavirus+Tweets.CSV and the preprocessing were borrowed from https://www.kaggle.com/satanizer/covid-19-tweets-analysis. It contains tweets which contain hashtags related to the Coronavirus. For computational speed we will analyze a dataset from one day: April 30, 2020. We encourage you to explore this dataset further and see how topics change over time.\n",
        "\n",
        "\n",
        "Fill in the cells provided marked `TODO` with code to answer the questions. **Unless otherwise noted, every answer you submit should have code that clearly shows the answer in the output.** Answers submitted that do not have associated code that shows the answer may not be accepted for credit. \n",
        "\n",
        "**Make sure to restart the kernel and run all cells** (especially before turning it in) to make sure your code runs correctly. Answer the questions on Gradescope and make sure to download this file once you've finished the assignment and upload it to Canvas as well.\n",
        "\n",
        "> Copyright ©2020 Valentina Staneva.  All rights reserved.  Permission is hereby granted to students registered for University of Washington CSE/STAT 416 for use solely during Spring 2020 for purposes of the course.  No other use, copying, distribution, or modification is permitted without prior written consent. Copyrights for third-party components of this work must be honored.  Instructors interested in reusing these course materials should contact the author.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEd_2KEiQAeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# string manipulation libraries\n",
        "import string\n",
        "import re\n",
        "\n",
        "# text processing libraries\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxEU6SuQxQHA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Data Loading\n",
        "\n",
        "First let's read the dataset into a data frame and have a look what is there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0BUuFxxNr0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "0d42bee2-7d63-41f6-8a0a-1cb0f8a28add"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/valentina-s/cse-stat-416-sp20/master/data/2020-04-30_Coronavirus_Tweets_small.csv')\n",
        "data.head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,4,5,6,7,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>is_quote</th>\n",
              "      <th>is_retweet</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>country_code</th>\n",
              "      <th>place_full_name</th>\n",
              "      <th>place_type</th>\n",
              "      <th>verified</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Asegura sus beneficios, registra a tu esposa e...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#COVID19 | El Faro conversó con policías, un f...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Si ya era cuestionable la burocracia, lo es má...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Las medidas de higiene ayudan a reducir la pro...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>38.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cubre tu nariz y boca al estornudar con el áng...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text is_quote  ... verified  lang\n",
              "0  Asegura sus beneficios, registra a tu esposa e...    False  ...    False    es\n",
              "1  #COVID19 | El Faro conversó con policías, un f...    False  ...     True    es\n",
              "2  Si ya era cuestionable la burocracia, lo es má...    False  ...    False    es\n",
              "3  Las medidas de higiene ayudan a reducir la pro...    False  ...     True    es\n",
              "4  Cubre tu nariz y boca al estornudar con el áng...    False  ...    False    es\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuJ3-u1AzEpP",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Text Preprocessing\n",
        "\n",
        "First, we will do several text preprocessing steps. We will:\n",
        "* limit to English language\n",
        "* remove URL links\n",
        "* make lower case\n",
        "* remove pronunciation\n",
        "* remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRkVrmV5OMqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select tweets in English\n",
        "text_en = data['text'][data['lang']=='en']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJSg5kFwPqiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove URL links\n",
        "text_en_lr = text_en.apply(lambda x: re.sub(r\"https\\S+\", \"\", str(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCnKhC7P5Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make lower case\n",
        "text_en_lr_lc = text_en_lr.apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7mT06_HP_rI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove punctuation\n",
        "text_en_lr_lc_pr = text_en_lr_lc.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2KfQmuFQvug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "41aee19a-c105-4982-d6cd-c4131a799a17"
      },
      "source": [
        "# remove stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v88F1frQaWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19','covid-19', 'covidー19'])\n",
        "\n",
        "text_en_lr_lc_pr_sr = text_en_lr_lc_pr.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEcQyxPOz7zO",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF Matrix\n",
        "\n",
        "Remember that matrix factorization methods work on matrices of numbers not text so we need to convert the text into a meaningful numeric representation. Earlier we discussed the Term Frequency-Inverse Document Frequency as a good way to do that since it defines a word weight vector for each document by accounting for the most popular words such as `the` or `a`.  We can extract it using `scikit-learn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90S6sBJcQo4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create TF-IDF matrix\n",
        "vectorizer = TfidfVectorizer(max_df=0.95)  # ignore words with very high doc frequency\n",
        "tf_idf = vectorizer.fit_transform(text_en_lr_lc_pr_sr)\n",
        "\n",
        "# exctract also the words so that we know which feature corresponds to which word\n",
        "feature_names = vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m8NLFFwUf3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf2d2e42-e398-43a6-d800-50cbefe17ffa"
      },
      "source": [
        "# check out the shape\n",
        "tf_idf.shape"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(198579, 255511)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qp1lzoU8m1F",
        "colab_type": "text"
      },
      "source": [
        "**Question 1** (enter answers on gradescope)\n",
        "\n",
        "What is the number of observations? What is the number of features? Which dimension are we going to reduce?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58C3bG-q4PQ",
        "colab_type": "text"
      },
      "source": [
        "### Non-negative Matrix Decomposition for Topic Discovery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TlfA2U29ONr",
        "colab_type": "text"
      },
      "source": [
        "Next we will use the [NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) method from `scikit-learn` to extract the topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyej3WnDUk-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipa6j9FDfXJ",
        "colab_type": "text"
      },
      "source": [
        "Set up an NMF model with 5 components. So that we all get the same results, please pass these parameters `init = 'nndsvd'` and `random_state = 1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_LuG5J6XKsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO \n",
        "# define an NMF model with `n_components = 5`\n",
        "nmf = NMF(n_components=5, init='nndsvd', random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug2HSiqoJ6Aa",
        "colab_type": "text"
      },
      "source": [
        "Now use the `nmf.fit` method to obtain the factorization. Note you do not need to change any default parameters, but you have to ensure your matrix is passed in the right format, i.e. `#observations x #features`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3CoPzi9XPVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "04808ddd-4720-47a2-d5ae-d31d8591b89e"
      },
      "source": [
        "# TODO fit NMF to the TF-IDF matrix\n",
        "nmf.fit(tf_idf)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
              "    n_components=5, random_state=1, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niEspTjRXSkW",
        "colab_type": "text"
      },
      "source": [
        "The topics are stored within the object `nmf.components_`. Now you can find the weight of each word within a topic. It will be interesting to look at the words corresponding to each topic ordered by their heighest weight. Remember the words corresponding to each topic are stored in `feature_names`, while the weights are stored in `nmf.components_`. You can use the [`argsort()`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) function to extract the indeces of the sorted words. Note that `argsort` sorts from lowest to heighest so you need to look at the last values for the ones with heighest weights. You can reverse a list/array with `[::-1]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl3pz-D8YgzF",
        "colab_type": "text"
      },
      "source": [
        "Find the maximum weight of a word in the first topic, and the word which corresponds to it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P02QUBnmZ-BS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e7faeae-696e-40a4-f1f4-a52790e41cc1"
      },
      "source": [
        "#TODO\n",
        "import numpy as np\n",
        "topics = nmf.components_\n",
        "max_weight = max(topics[0])\n",
        "max_word = feature_names[np.argmax(topics[0])]\n",
        "print(\"max weight:\",max_weight,\"\\nmax_word:\",max_word)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max weight: 2.178419059331134 \n",
            "max_word: people\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js1uI2uQd4fJ",
        "colab_type": "text"
      },
      "source": [
        "**Question 2.1-2.2**\n",
        "\n",
        "What is the maximum weight of a word in the first topic? What is the word associated with it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXShnIs7Yex6",
        "colab_type": "text"
      },
      "source": [
        "Create a function `words_from_topic` to extract an ordered list of words in a topic (highest weight first)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVDXmfvEnSy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO\n",
        "def words_from_topic(topic, feature_names):\n",
        "  # write a loop to create the list of ordered words (highest weight first)\n",
        "  # (you can use a list comprehension if you are familiar with them)\n",
        "  ordered_words = [feature_names[i] for i in np.argsort(topic)[::-1]]\n",
        "  return(ordered_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44YDL2Rto9Ja",
        "colab_type": "text"
      },
      "source": [
        "Now you can use the function below to look at all the topic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHoAl8gSX8nF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_top_words(components, feature_names, n_top_words):\n",
        "    \"\"\" \n",
        "    print_top_words prints the first n_top_words for each topic in components\n",
        "    \"\"\"\n",
        "    for topic_idx, topic in enumerate(components):\n",
        "        ordered_words = words_from_topic(topic, feature_names)\n",
        "        message = \"Topic #%d: \" % (topic_idx+1)\n",
        "        message += \", \".join(ordered_words[:n_top_words])\n",
        "        print(message)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL2y0wFuYN6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c2366280-29c3-4f39-ba57-0b481c9ef9ae"
      },
      "source": [
        "print_top_words(nmf.components_, feature_names, 10)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic #1: people, lockdown, get, home, stay, like, one, time, know, day\n",
            "Topic #2: cases, new, deaths, total, confirmed, reported, number, positive, reports, today\n",
            "Topic #3: help, spread, app, selfreporting, symptoms, download, sooner, identify, slow, daily\n",
            "Topic #4: us, china, join, trump, let, drug, says, million, realdonaldtrump, deaths\n",
            "Topic #5: pandemic, health, support, help, crisis, workers, global, news, read, response\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "borQJNFxp2YN",
        "colab_type": "text"
      },
      "source": [
        "**Question 2.3:** (answer on Gradescope)\n",
        "\n",
        "What is the 4th word of the 4th topic. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAwds09LkXC8",
        "colab_type": "text"
      },
      "source": [
        "Next let's look at a specific tweet and the individual contributions of the topics. For that we need to look at the coordinates of the transformed original tf-idf features. That can be obtained through `nmf.fit_transform` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XKtE20lPEJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO\n",
        "tweets_projected = nmf.fit_transform(tf_idf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBPHk2c7kmFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbfaa065-b6f8-44bc-a638-2ac01206c002"
      },
      "source": [
        "tweets_projected.shape"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(198579, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kV0M40RgIuh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "15924e25-b726-4e1c-ae2c-b3d7bcf0db7b"
      },
      "source": [
        "text_en_lr_lc_pr_sr.iloc[0]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'attention seattle shoppers grocery stores working hard keep employees customers safe part help slow spread ☑️ limit trips ☑️ respect special shopping hours ☑️ follow socialdistance guidance stores wegotthisseattle'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxRUu0UWhAsb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96b3bdd9-af8a-4e25-efb3-f9f46b68af12"
      },
      "source": [
        "# TODO find the weight of topic 3 in the first tweet\n",
        "tweets_projected[0,2]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02597223727522194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ0ckLBiZTIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecd969e0-0af3-40fb-bd40-cedf552123b7"
      },
      "source": [
        "text_en_lr_lc_pr_sr.iloc[1]"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'microsoft sees digital reboot pandemic profits'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaeTuuti8nQv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "50ab4abe-18dd-4a53-c1ea-2ef2541197d8"
      },
      "source": [
        "#TODO look at the weights for the second tweet and decide which topic it is associated with\n",
        "print(\"weights:\", tweets_projected[1])\n",
        "print(\"-- most associated with topic\", np.argmax(tweets_projected[1])+1)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights: [0.         0.         0.         0.         0.02828405]\n",
            "-- most associated with topic 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB8SpbQihFgs",
        "colab_type": "text"
      },
      "source": [
        "**Question 3.1:** (answer on Gradescope)\n",
        "\n",
        "What is the weight of topic 3 in the first tweet?\n",
        "\n",
        "**Question 3.2:** (answer on Gradescope)\n",
        "\n",
        "Which topic is tweet 2 associated with?"
      ]
    }
  ]
}